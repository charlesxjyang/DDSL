{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(2)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random hull\n",
    "def rand_hull(n_points, dim, device=\"cuda\", dtype=torch.float32):\n",
    "    V = np.random.rand(n_points, dim)\n",
    "    mesh = Delaunay(V)\n",
    "    E = mesh.simplices\n",
    "    E = torch.LongTensor(E).to(device)\n",
    "\n",
    "    # normalize V, and create V as leaf tensor\n",
    "    V_bb = np.max(V, axis=0) - np.min(V, axis=0)\n",
    "    V_c = (np.max(V, axis=0) + np.min(V, axis=0)) / 2\n",
    "    V -= V_c\n",
    "    V /= 1.5*V_bb\n",
    "    V += 0.5\n",
    "    V = torch.tensor(V, device=device, dtype=dtype, requires_grad=True)\n",
    "    return V, E\n",
    "\n",
    "def plot_F(F):\n",
    "    F = F.squeeze()\n",
    "    if torch.sum(torch.isnan(F)) > 0:\n",
    "        nnan = torch.sum(torch.isnan(F))\n",
    "        F[torch.isnan(F)] = 0\n",
    "        print(\"Padding {} nan terms to zero.\".format(nnan))\n",
    "\n",
    "    F_ = F.cpu().detach().numpy()\n",
    "    f_ = torch.irfft(F, dim, signal_sizes=res).squeeze().cpu().detach().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(f_.T, origin='lower')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from math import pi\n",
    "\n",
    "from utils import fftfreqs, simplex_content, triangulate_interior, permute_seq, coalesce_update\n",
    "from math import ceil, factorial\n",
    "\n",
    "class SimplexFT(Function):\n",
    "    \"\"\"\n",
    "    Fourier transform for signal defined on a j-simplex set in R^n space\n",
    "    :param V: vertex tensor. float tensor of shape (n_vertex, n_dims)\n",
    "    :param E: element tensor. int tensor of shape (n_elem, j or j+1)\n",
    "              if j cols, triangulate/tetrahedronize interior first.\n",
    "    :param D: int ndarray of shape (n_elem, n_channel)\n",
    "    :param res: n_dims int tuple of number of frequency modes\n",
    "    :param t: n_dims tuple of period in each dimension\n",
    "    :param j: dimension of simplex set\n",
    "    :param mode: normalization mode.\n",
    "                 'density' for preserving density, 'mass' for preserving mass\n",
    "    :return: F: ndarray of shape (res[0], res[1], ..., res[-1]/2, n_channel)\n",
    "                last dimension is halfed since the signal is assumed to be real\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, V, E, D, res, t, j, elem_batch=100, mode='density'):\n",
    "        ## boiler-plate\n",
    "        ctx.mark_non_differentiable(E, D) # mark non-differentiable\n",
    "        ctx.res = res\n",
    "        ctx.t = t\n",
    "        ctx.j = j\n",
    "        ctx.mode = mode\n",
    "        ctx.n_dims = V.shape[1]\n",
    "        ctx.elem_batch = elem_batch\n",
    "        # compute content array\n",
    "        C = factorial(j) * simplex_content(V, E) # [n_elem, 1]\n",
    "        ctx.save_for_backward(V, E, D, C)\n",
    "\n",
    "        ## compute frequencies F\n",
    "        n_dims = ctx.n_dims\n",
    "        assert(n_dims == len(res))  # consistent spacial dimensionality\n",
    "        assert(E.shape[0] == D.shape[0])  # consistent vertex numbers\n",
    "        assert(mode in ['density', 'mass'])\n",
    "\n",
    "        # number of columns in E\n",
    "        subdim = E.shape[1] == j and n_dims == j\n",
    "        assert (E.shape[1] == j+1 or subdim)\n",
    "        if subdim:\n",
    "            E = triangulate_interior(V, E)\n",
    "        n_elem = E.shape[0]\n",
    "        n_vert = V.shape[0]\n",
    "        n_channel = D.shape[1]\n",
    "\n",
    "        # frequency tensor\n",
    "        omega = fftfreqs(res, dtype=V.dtype).to(V.device) # [dim0, dim1, dim2, d]\n",
    "\n",
    "        # normalize frequencies\n",
    "        for dim in range(n_dims):\n",
    "            omega[..., dim] *= 2 * pi / t[dim]\n",
    "\n",
    "        # initialize output F\n",
    "        F_shape = list(omega.shape)[:-1]\n",
    "        F_shape += [n_channel, 2]\n",
    "        F = torch.zeros(*F_shape, dtype=V.dtype, device=V.device) # [dimX, dimY, dimZ, n_chan, 2] 2: real/imag\n",
    "\n",
    "        # compute element-point tensor\n",
    "        P = V[E] # [n_elem, j+1, d]\n",
    "\n",
    "        # loop over element batches\n",
    "        for idx in range(ceil(n_elem/elem_batch)):\n",
    "            id_start = idx * elem_batch\n",
    "            id_end = min((idx+1) * elem_batch, n_elem)\n",
    "            Xi = P[id_start:id_end] # [elem_batch, j+1, d]\n",
    "            Di = D[id_start:id_end] # [elem_batch, n_channel]\n",
    "            Ci = C[id_start:id_end] # [elem_batch, 1]\n",
    "            CDi = Ci * Di # [elem_batch, n_channel]\n",
    "            sig = torch.einsum('bjd,...d->bj...', (Xi, omega)) \n",
    "            sig = torch.unsqueeze(sig, dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1]\n",
    "            esig = torch.stack((torch.cos(sig), -torch.sin(sig)), dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 2]\n",
    "            sig = torch.unsqueeze(sig, dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "            denom = torch.ones_like(sig) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "            for dim in range(1, j+1):\n",
    "                seq = permute_seq(dim, j+1)\n",
    "                denom *= sig - sig[:, seq]\n",
    "            tmp = torch.sum(esig / denom, dim=1) # [elem_batch, dimX, dimY, dimZ, 1, 2]\n",
    "            CDi.unsqueeze_(-1) # [elem_batch, n_channel, 1]\n",
    "            for _ in range(n_dims): # unsqueeze to broadcast\n",
    "                CDi.unsqueeze_(dim=1) # [elem_batch, 1, 1, 1, n_channel, 1]\n",
    "            tmp *= CDi # [elem_batch, dimX, dimY, dimZ, n_channel, 2]\n",
    "            Fi = torch.sum(tmp, dim=0, keepdim=False) # [dimX, dimY, dimZ, n_channel, 2]\n",
    "            Fi[tuple([0] * n_dims)] = - 1 / factorial(j) * torch.sum(CDi, dim=0).unsqueeze(dim=-1)\n",
    "            F += Fi\n",
    "        # multiply by (i)**j\n",
    "        if j == 0:\n",
    "            pass\n",
    "        elif j == 1:\n",
    "            F = F[..., [1, 0]]\n",
    "            F[..., 0] = -F[..., 0]\n",
    "        elif j == 2:\n",
    "            F *= -1\n",
    "        elif j == 3:\n",
    "            F = F[..., [1, 0]]\n",
    "            F[..., 1] = -F[..., 1]\n",
    "\n",
    "        if mode == 'density':\n",
    "            if not np.array_equal(res, res[0]*np.ones(len(res))):\n",
    "                print(\"WARNING: density preserving mode not correctly implemented if not all res are equal\")\n",
    "            F *= res[0] ** j\n",
    "        return F\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, dF):\n",
    "        \"\"\"\n",
    "        :param dF: per-frequency sensitivity from downstream layers of shape [dimX, dimY, dimZ, n_channel, 2]\n",
    "        \"\"\"\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            V, E, D, C = ctx.saved_tensors\n",
    "\n",
    "            n_dims = V.shape[1]\n",
    "            assert(n_dims == 2) # backwards not implemented for other dims yet\n",
    "            assert(ctx.j == 2) # not yet implemented for other simplices\n",
    "\n",
    "            n_elem = E.shape[0]\n",
    "            n_vert = V.shape[0]\n",
    "            n_channel = D.shape[1]\n",
    "\n",
    "            # recover context\n",
    "            res = ctx.res\n",
    "            t = ctx.t\n",
    "            j = ctx.j\n",
    "            mode = ctx.mode\n",
    "            n_dims = ctx.n_dims\n",
    "            elem_batch = ctx.elem_batch\n",
    "\n",
    "            # frequency tensor\n",
    "            omega = fftfreqs(res, dtype=V.dtype).to(V.device) # [dim0, dim1, dim2, d]\n",
    "\n",
    "            # normalize frequencies\n",
    "            for dim in range(n_dims):\n",
    "                omega[..., dim] *= 2 * pi / t[dim]\n",
    "\n",
    "            # compute element-point tensor\n",
    "            P = V[E] # [n_elem, j+1, d]\n",
    "\n",
    "            # initialize output dV\n",
    "            dV = torch.zeros_like(V) # [j+1, n_dims]\n",
    "\n",
    "            # compute element-point tensor\n",
    "            P = V[E] # [n_elem, j+1, d]\n",
    "\n",
    "            # helper functions\n",
    "            seq = lambda i : permute_seq(i, j+1) # return looped sequences\n",
    "            def img(x): # tensor times i (assume last dim: real/imag)\n",
    "                res = x[..., [1, 0]]\n",
    "                res[..., 0] = -res[..., 0]\n",
    "                return res\n",
    "\n",
    "            # loop over element batches\n",
    "            for idx in range(ceil(n_elem/elem_batch)):\n",
    "                id_start = idx * elem_batch\n",
    "                id_end = min((idx+1) * elem_batch, n_elem)\n",
    "                elem_batch_i = id_end - id_start\n",
    "                Xi = P[id_start:id_end] # [elem_batch, j+1, d]\n",
    "                Di = D[id_start:id_end] # [elem_batch, n_channel]\n",
    "                Ci = C[id_start:id_end] # [elem_batch, 1]\n",
    "                Ei = E[id_start:id_end] # [elem_batch, j+1]\n",
    "                CDi = Ci * Di # [elem_batch, n_channel]\n",
    "                sig = torch.einsum('bjd,...d->bj...', (Xi, omega)) \n",
    "                sig = torch.unsqueeze(sig, dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1]\n",
    "                esig = torch.stack((torch.cos(sig), -torch.sin(sig)), dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 2]\n",
    "                # specific code for j == 2 and dim == 2 TODO: Add other dimensions\n",
    "                s01 = 1/(sig - sig[:, seq(1)]).unsqueeze(-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "                s20 = 1/(sig[:, seq(2)] - sig).unsqueeze(-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "                tmp = esig * s01 * s20 # [elem_batch, j+1, dimX, dimY, dimZ, 1, 2]\n",
    "                S = -torch.sum(tmp, dim=1) # [elem_batch, dimX, dimY, dimZ, 1, 2]\n",
    "                S[:, 0, 0] = -1 / factorial(j) # HARDCODE dim==2\n",
    "                tmp = img(tmp) + tmp*(s01 - s20) + tmp[:, seq(1)]*s01 - tmp[:, seq(2)]*s20 # [elem_batch, j+1, dimX, dimY, dimZ, 1, 2]\n",
    "                tmp.unsqueeze_(-3) # [elem_batch, j+1, dimX, dimY, dimZ, 1(n_dims), 1(n_channel), 2(real/imag)]\n",
    "                tmp[:, :, 0, 0] = 0\n",
    "                tmp = tmp * dF.unsqueeze(-3) * omega.unsqueeze(-1).unsqueeze(-1) # [elem_batch, j+1, dimX, dimY, dimZ, n_dims, n_channel, 2]\n",
    "                tmp = tmp.view(elem_batch_i, j+1, -1, n_dims, n_channel, 2).sum(2) # [elem_batch, j+1, n_dims, n_channel, 2]\n",
    "                tmp = -(tmp * CDi.unsqueeze(1).unsqueeze(1).unsqueeze(-1)).view(elem_batch_i, j+1, n_dims, -1).sum(-1) # [elem_batch, j+1, n_dims]\n",
    "                xy = torch.stack((Xi[:, seq(1), 1] - Xi[:, seq(2), 1], - Xi[:, seq(1), 0] + Xi[:, seq(2), 0]), dim=-1) # [elem_batch, j+1, n_dims]\n",
    "                tmp2 = S * dF # [elem_batch, dimX, dimY, dimZ, n_channel, 2]\n",
    "                tmp2 = tmp2.sum(-1).view(elem_batch_i, -1, n_channel).sum(1) # [elem_batch, n_channel]\n",
    "                tmp2 = -((tmp2 * Di).sum(-1).unsqueeze(-1).unsqueeze(-1) * xy) # [elem_batch, j+1, n_dims]\n",
    "                ddV = coalesce_update(Ei, tmp + tmp2, dV.shape)\n",
    "                dV += ddV\n",
    "            if mode == \"density\":\n",
    "                dV *= res[0] ** j\n",
    "        else:\n",
    "            dV = None\n",
    "            \n",
    "        return dV, None, None, None, None, None, None, None\n",
    "\n",
    "    \n",
    "class DDSL(nn.Module):\n",
    "    def __init__(self, res, t, j, elem_batch=100, mode='density'):\n",
    "        super(DDSL, self).__init__()\n",
    "        self.res = res\n",
    "        self.t = t\n",
    "        self.j = j\n",
    "        self.elem_batch = elem_batch\n",
    "        self.mode = mode\n",
    "    def forward(self, V, E, D):\n",
    "        return SimplexFT.apply(V,E,D,self.res,self.t,self.j,self.elem_batch,self.mode)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!]Warning: zeroing small negative number tensor([ 1.8713e-06,  5.9443e-06,  1.6168e-05,  3.8286e-07,  2.5964e-05,\n",
      "         4.3660e-06,  0.0000e+00,  2.8821e-06,  5.3140e-06,  2.9441e-05,\n",
      "         5.2247e-07,  4.5550e-05,  2.6005e-06,  5.5873e-06,  7.4973e-07,\n",
      "         6.0697e-06, -1.7297e-04,  1.5708e-06,  1.3029e-05,  8.0025e-06,\n",
      "         6.2355e-07,  2.3916e-06,  1.9990e-05,  7.6866e-05,  1.8350e-05,\n",
      "         8.2832e-07,  4.3205e-06,  1.3456e-06,  1.8972e-06,  9.1140e-06,\n",
      "         6.6077e-06,  2.0623e-06,  4.4150e-06,  7.4349e-06,  9.0356e-06,\n",
      "         1.0471e-04,  2.8621e-06,  3.8440e-05,  1.4201e-05,  5.3398e-06,\n",
      "         3.3683e-07, -4.9650e-05, -1.4587e-06,  2.7960e-06,  5.0406e-07,\n",
      "         5.1494e-08,  2.9612e-05,  6.8145e-06,  1.9210e-07,  2.1702e-06,\n",
      "         4.1765e-06,  4.1937e-07,  4.0861e-07,  1.2375e-07,  2.7657e-06,\n",
      "         2.1998e-07,  1.8195e-05,  2.5549e-06,  3.9509e-06,  5.7593e-07,\n",
      "         8.0747e-08,  1.0839e-07,  1.4752e-06,  1.8889e-05,  1.5462e-07,\n",
      "         4.2252e-06,  1.1644e-05,  7.9006e-06,  2.3596e-05,  9.7056e-08,\n",
      "        -2.2560e-06,  2.7060e-05,  1.0403e-06,  2.1978e-07,  1.6825e-06,\n",
      "         3.2657e-07,  2.0064e-06,  9.1074e-06,  4.6273e-05,  1.1101e-07,\n",
      "         1.0447e-05,  5.5187e-05,  1.6018e-06,  4.4914e-06,  8.0179e-07,\n",
      "         5.7626e-08,  2.1970e-05,  8.0893e-08,  6.9946e-07,  8.8879e-08,\n",
      "         1.0017e-07,  3.0931e-08,  1.1057e-06,  5.1051e-10,  4.8073e-07,\n",
      "         2.7414e-08,  3.7864e-09,  4.6397e-06,  7.7735e-06,  7.7365e-06,\n",
      "        -1.8732e-05, -3.6039e-07,  1.6525e-05,  2.3287e-07,  1.0379e-05,\n",
      "         4.4069e-06,  2.7616e-05,  8.7529e-06,  5.9626e-06,  4.1217e-06,\n",
      "         6.9492e-07,  1.0756e-07,  3.6269e-05,  1.1136e-05,  2.2003e-05,\n",
      "         1.4833e-05,  3.1790e-06,  6.4518e-06,  6.3761e-06,  5.9165e-06,\n",
      "         3.5353e-05,  1.0752e-05,  1.3163e-06,  2.1904e-05,  6.3392e-06,\n",
      "         4.4850e-06,  4.2987e-06,  1.2645e-05,  6.1519e-06,  2.0792e-06,\n",
      "         7.8981e-06,  4.8286e-06,  9.8718e-06,  8.6733e-06,  3.1022e-06,\n",
      "         5.5205e-07,  1.8918e-06,  1.0450e-07,  3.0909e-05,  2.6931e-05,\n",
      "         2.6215e-07,  2.8404e-06,  9.2262e-05,  3.4970e-06,  3.8779e-06,\n",
      "         3.2999e-06,  6.2307e-07,  2.1870e-07,  9.9343e-07,  1.8977e-08,\n",
      "         1.3638e-07,  1.1883e-06,  2.4150e-07,  5.4041e-07,  3.7893e-06,\n",
      "         6.9760e-07,  1.9065e-05,  7.9662e-06,  9.1787e-06,  7.1934e-06,\n",
      "         3.6383e-07,  7.1375e-08,  8.7266e-07,  1.5179e-07,  9.6627e-07,\n",
      "         7.5976e-07,  2.9610e-07,  4.7681e-07,  1.0968e-05,  4.2548e-06,\n",
      "         1.0544e-06,  1.5450e-06,  3.8785e-06,  1.2176e-06,  1.3209e-06,\n",
      "         6.5452e-06,  3.7638e-06,  1.5375e-06,  9.8923e-07,  9.0247e-06,\n",
      "         9.9041e-06,  5.6447e-06,  2.5276e-06,  1.4891e-05,  2.7073e-06,\n",
      "         1.5534e-06,  2.9213e-06,  2.3276e-05,  2.3241e-07], device='cuda:2')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [100, 256, 129, 1, 2] doesn't match the broadcast shape [100, 256, 129, 100, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e7ee936775f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddsl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Elements: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/envs/pytorch1/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-894357bc0c1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, V, E, D)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSimplexFT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melem_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-894357bc0c1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, V, E, D, res, t, j, elem_batch, mode)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# unsqueeze to broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mCDi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [elem_batch, 1, 1, 1, n_channel, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mCDi\u001b[0m \u001b[0;31m# [elem_batch, dimX, dimY, dimZ, n_channel, 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mFi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [dimX, dimY, dimZ, n_channel, 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mFi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [100, 256, 129, 1, 2] doesn't match the broadcast shape [100, 256, 129, 100, 2]"
     ]
    }
   ],
   "source": [
    "r = 256\n",
    "j = 2\n",
    "dim = 2\n",
    "npt = 100\n",
    "# Vertex matrix\n",
    "V, E = rand_hull(npt, dim, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "# Density matrix (Random Density 0 to 1)\n",
    "D = torch.rand(E.shape[0], 1, dtype=V.dtype, device=V.device)\n",
    "# D = torch.ones(E.shape[0], 1, dtype=V.dtype, device=V.device)\n",
    "\n",
    "res = [r] * 2\n",
    "t = [1] * 2\n",
    "\n",
    "# Sensitivity on each frequency mode\n",
    "dF = torch.ones(r, int(r/2)+1, 2, dtype=V.dtype) # unit sensitivity\n",
    "\n",
    "ddsl = DDSL(res, t, j, elem_batch=100, mode='density')\n",
    "\n",
    "t0 = time()\n",
    "F = ddsl(V, E, D)\n",
    "t1 = time()\n",
    "print(\"Number of Elements: {}\".format(E.shape[0]))\n",
    "print(\"Forward Time Lapse: {}\".format(t1 - t0))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# plot\n",
    "plot_F(F)\n",
    "\n",
    "# Backwards pass time\n",
    "loss = F.view(-1).sum(0)\n",
    "if V.grad is not None:\n",
    "    V.grad.zero_()\n",
    "t0 = time()\n",
    "loss.backward()\n",
    "t1 = time()\n",
    "dV = V.grad\n",
    "print(\"Backward Time Lapse: {}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LU = torch.rand(10,3,3)\n",
    "det_LU = torch.einsum('...ii->...i', (LU)).prod(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 256\n",
    "j = 2\n",
    "# # Vertex matrix\n",
    "# V_ = np.array([[0.2,0.2],\n",
    "#                [0.8,0.5],\n",
    "#                [0.8,0.8],\n",
    "#                [0.6,0.8]])\n",
    "# V_ += 1e-4*np.random.rand(*V_.shape)\n",
    "# V = torch.tensor(V_, dtype=torch.float64, requires_grad=True, device=\"cuda\")\n",
    "# # Element matrix\n",
    "# E = torch.LongTensor([[0,1,2],\n",
    "#                       [2,3,0]])\n",
    "# # Density matrix (Unit Density)\n",
    "# D = torch.ones(E.shape[0], 1, dtype=V.dtype)\n",
    "\n",
    "# Vertex matrix\n",
    "V, E = rand_hull(200, 2, device=\"cuda\", dtype=torch.float64)\n",
    "\n",
    "# Density matrix (Random Density 0 to 1)\n",
    "# D = torch.rand(E.shape[0], 1, dtype=V.dtype, device=V.device)\n",
    "D = torch.ones(E.shape[0], 1, dtype=V.dtype)\n",
    "\n",
    "res = [r] * 2\n",
    "t = [1] * 2\n",
    "\n",
    "E, D = E.cuda(), D.cuda()\n",
    "\n",
    "# Sensitivity on each frequency mode\n",
    "dF = torch.ones(r, int(r/2)+1, 2, dtype=V.dtype) # unit sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Adjoint solution\n",
    "ddsl = DDSL(res,t,j,100,'density')\n",
    "F = ddsl(V,E,D)\n",
    "loss = F.view(-1).sum(0)\n",
    "if V.grad is not None:\n",
    "    V.grad.zero_()\n",
    "loss.backward()\n",
    "dV = V.grad\n",
    "print(\"Analytical Gradient:\")\n",
    "print(dV)\n",
    "\n",
    "print(\"Finite Difference Gradient (New):\")\n",
    "# Finite difference approximation\n",
    "delta = 1e-6\n",
    "dV_fd_all = torch.zeros(*(list(V.shape)+list(dF.shape)+[2]), dtype=V.dtype).cuda()\n",
    "for ii in range(V.shape[0]):\n",
    "    for jj in range(V.shape[1]):\n",
    "        V_p = V.clone()\n",
    "        V_m = V.clone()\n",
    "        V_p[ii, jj] += delta\n",
    "        V_m[ii, jj] -= delta\n",
    "        Freq_p = ddsl(V_p, E, D)\n",
    "        Freq_m = ddsl(V_m, E, D)\n",
    "        dV_fd_all[ii,jj] = (Freq_p - Freq_m) / delta / 2\n",
    "dV_fd = dV_fd_all.view(V.shape[0], V.shape[1], -1).sum(-1)\n",
    "print(dV_fd)\n",
    "# print(dV/dV_fd)\n",
    "print(dV / dV_fd)\n",
    "plot_F(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forward code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsl = DDSL(res, t, j, elem_batch=100, mode='density')\n",
    "\n",
    "dim = 2\n",
    "t0 = time()\n",
    "F = ddsl(V, E, D)\n",
    "print(\"Time Lapse: {}\".format(time() - t0))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# plot\n",
    "F = F.squeeze()\n",
    "if torch.sum(torch.isnan(F)) > 0:\n",
    "    nnan = torch.sum(torch.isnan(F))\n",
    "    F[torch.isnan(F)] = 0\n",
    "    print(\"Padding {} nan terms to zero.\".format(nnan))\n",
    "\n",
    "F_ = F.cpu().detach().numpy()\n",
    "f_ = torch.irfft(F, dim, signal_sizes=res).squeeze().cpu().detach().numpy()\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(f_.T, origin='lower')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test backward code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
