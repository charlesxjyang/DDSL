{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_shape(dim, j):\n",
    "    assert(j in [0, 1, 2, 3])\n",
    "    assert(dim in [2, 3])\n",
    "    assert(j <= dim)\n",
    "    if dim == 3:\n",
    "        V = [[1, 1, 1],\n",
    "             [1, -1, -1],\n",
    "             [-1, 1, -1],\n",
    "             [-1, -1, 1]]\n",
    "        if j == 0:\n",
    "            E = [[0],\n",
    "                 [1],\n",
    "                 [2],\n",
    "                 [3]]\n",
    "        elif j == 1:\n",
    "            E = [[0,1],\n",
    "                 [0,2],\n",
    "                 [0,3],\n",
    "                 [1,2],\n",
    "                 [1,3],\n",
    "                 [2,3]]\n",
    "        elif j == 2:\n",
    "            E = [[0,1,2],\n",
    "                 [0,1,3],\n",
    "                 [0,2,3],\n",
    "                 [1,2,3]]\n",
    "        elif j == 3:\n",
    "            E = [[0,1,2,3]]\n",
    "    elif dim == 2:\n",
    "        V = [[.25, .25],\n",
    "             [.75, .25],\n",
    "             [.50, .75]]\n",
    "        if j == 0:\n",
    "            E = [[0],\n",
    "                 [1],\n",
    "                 [2]]\n",
    "        elif j == 1:\n",
    "            E = [[0,1],\n",
    "                 [0,2],\n",
    "                 [1,2]]\n",
    "        elif j == 2:\n",
    "            E = [[0,1,2]]\n",
    "    V = torch.FloatTensor(V)\n",
    "    E = torch.LongTensor(E)\n",
    "    \n",
    "    # normalize V\n",
    "    V_bb = torch.max(V, dim=0)[0] - torch.min(V, dim=0)[0]\n",
    "    V_c = (torch.max(V, dim=0)[0] + torch.min(V, dim=0)[0]) / 2\n",
    "    V -= V_c\n",
    "    V /= 1.5*V_bb\n",
    "    V += 0.5\n",
    "    print(V_bb)\n",
    "    return V, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random hull\n",
    "def rand_hull(n_points, dim):\n",
    "    V = np.random.rand(n_points, dim)\n",
    "    mesh = Delaunay(V)\n",
    "    E = mesh.simplices\n",
    "    return torch.FloatTensor(V), torch.LongTensor(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplex_content(V, E):\n",
    "    \"\"\"\n",
    "    Compute the content of simplices in a simplicial complex\n",
    "    :param V: vertex tensor. float tensor of shape (n_vertex, n_dims)\n",
    "    :param E: element tensor. int tensor of shape (n_elem, j+1)\n",
    "    \"\"\"\n",
    "    ne = E.shape[0] # number of elements\n",
    "    nppe = E.shape[1] # number of points per element\n",
    "    assert(nppe in [1, 2, 3, 4]) # points, lines, tri or tet\n",
    "    if nppe == 1: # points\n",
    "        return torch.ones(ne, 1, dtype=V.dtype)\n",
    "    if nppe == 2: # lines\n",
    "        P = V[E]\n",
    "        Len = torch.norm(P[:, 1:] - P[:, :-1], dim=-1)\n",
    "        return Len\n",
    "    elif nppe == 3: # triangles\n",
    "        E_ = torch.cat([E, E[:, 0:1]], dim=-1)\n",
    "        P = V[E_]\n",
    "        L = torch.norm(P[:, 1:] - P[:, :-1], dim=-1)\n",
    "        S = torch.sum(L, dim=-1, keepdim=True) / 2\n",
    "        Area = torch.sqrt(S*(S-L[:, 0:1])*(S-L[:, 1:2])*(S-L[:, 2:])) # Heron's Formula\n",
    "        return Area\n",
    "    elif nppe == 4: # tetrahedron\n",
    "        P = V[E]\n",
    "        Va = P[:, 1] - P[:, 0]\n",
    "        Vb = P[:, 2] - P[:, 0]\n",
    "        Vc = P[:, 3] - P[:, 0]\n",
    "        Vol = torch.abs(torch.einsum('ab,ab->a', (Va, torch.cross(Vb, Vc, dim=-1))) / 6)\n",
    "        return Vol\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "from utils import fftfreqs, simplex_content, triangulate_interior, permute_seq\n",
    "from math import ceil, factorial\n",
    "\n",
    "\n",
    "class DDSL(Function):\n",
    "    \"\"\"\n",
    "    Fourier transform for signal defined on a j-simplex set in R^n space\n",
    "    :param V: vertex tensor. float tensor of shape (n_vertex, n_dims)\n",
    "    :param E: element tensor. int tensor of shape (n_elem, j or j+1)\n",
    "              if j cols, triangulate/tetrahedronize interior first.\n",
    "    :param D: int ndarray of shape (n_vertex, n_channel)\n",
    "    :param res: n_dims int tuple of number of frequency modes\n",
    "    :param t: n_dims tuple of period in each dimension\n",
    "    :param j: dimension of simplex set\n",
    "    :param mode: normalization mode.\n",
    "                 'density' for preserving density, 'mass' for preserving mass\n",
    "    :return: F: ndarray of shape (res[0], res[1], ..., res[-1]/2, n_channel)\n",
    "                last dimension is halfed since the signal is assumed to be real\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, V, E, D, res, t, j, elem_batch=100, mode='density'):\n",
    "        ## boiler-plate\n",
    "        ctx.res = res\n",
    "        ctx.t = t\n",
    "        ctx.j = j\n",
    "        ctx.mode = 1 if mode == 'density' else 0\n",
    "        ctx.save_for_backward(V, E, D)\n",
    "\n",
    "        ## compute frequencies F\n",
    "        n_dims = V.shape[1]\n",
    "        assert(n_dims == len(res))  # consistent spacial dimensionality\n",
    "        assert(E.shape[0] == D.shape[0])  # consistent vertex numbers\n",
    "        assert(mode in ['density', 'mass'])\n",
    "\n",
    "        # number of columns in E\n",
    "        subdim = E.shape[1] == j and n_dims == j\n",
    "        assert (E.shape[1] == j+1 or subdim)\n",
    "        if subdim:\n",
    "            E = triangulate_interior(V, E)\n",
    "        n_elem = E.shape[0]\n",
    "        n_vert = V.shape[0]\n",
    "        n_channel = D.shape[1]\n",
    "\n",
    "        # frequency tensor\n",
    "        omega = fftfreqs(res).to(V.device) # [dim0, dim1, dim2, d]\n",
    "        omega[tuple([0] * n_dims)] += 1e-3 # will get rid of this\n",
    "\n",
    "        # normalize frequencies\n",
    "        for dim in range(n_dims):\n",
    "            omega[..., dim] *= 2 * np.pi / t[dim]\n",
    "\n",
    "        # initialize output F\n",
    "        F_shape = list(omega.shape)[:-1]\n",
    "        F_shape += [n_channel, 2]\n",
    "        F = torch.zeros(*F_shape, dtype=torch.float32, device=V.device) # [dimX, dimY, dimZ, n_chan, 2] 2: real/imag\n",
    "\n",
    "        # compute content array\n",
    "        C = simplex_content(V, E) # [n_elem, 1]\n",
    "        print(C.shape)\n",
    "\n",
    "        # compute element-point tensor\n",
    "        P = V[E] # [n_elem, j+1, d]\n",
    "\n",
    "        # loop over element batches\n",
    "        for idx in range(ceil(n_vert/elem_batch)):\n",
    "            id_start = idx * elem_batch\n",
    "            id_end = min((idx+1) * elem_batch, n_elem)\n",
    "            X = P[id_start:id_end] # [elem_batch, j+1, d]\n",
    "            Di = D[id_start:id_end] # [elem_batch, n_channel]\n",
    "            Ci = C[id_start:id_end] # [elem_batch, 1]\n",
    "            CDi = Ci * Di # [elem_batch, n_channel]\n",
    "            sig = torch.einsum('bjd,...d->bj...', (X, omega)) \n",
    "            sig = torch.unsqueeze(sig, dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1]\n",
    "            sig_sub = [[None] * (n_dims+1)] * (n_dims+1)\n",
    "            esig = torch.stack((torch.cos(sig), torch.sin(sig)), dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 2]\n",
    "            sig = torch.unsqueeze(sig, dim=-1) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "            denom = torch.ones_like(sig) # [elem_batch, j+1, dimX, dimY, dimZ, 1, 1]\n",
    "            for dim in range(1, j+1):\n",
    "                seq = permute_seq(dim, j+1)\n",
    "                denom *= sig - sig[:, seq]\n",
    "            tmp = torch.sum(esig / denom, dim=1) # [elem_batch, dimX, dimY, dimZ, 1, 2]\n",
    "            CDi.unsqueeze_(-1)\n",
    "            for _ in range(n_dims):\n",
    "                CDi.unsqueeze_(dim=1) # [elem_batch, 1, 1, 1, n_channel, 1]\n",
    "            tmp *= CDi # [elem_batch, dimX, dimY, dimZ, n_channel, 2]\n",
    "            Fi = torch.sum(tmp, dim=0, keepdim=False) # [dimX, dimY, dimZ, n_channel, 2]\n",
    "            Fi[tuple([0] * n_dims)] = - 1 / factorial(j) * torch.sum(CDi, dim=0).unsqueeze(dim=-1)\n",
    "            # Fi *= 1j**j # [dimX, dimY, dimZ, n_chan, 2] 2: real/imag\n",
    "            if j == 0:\n",
    "                pass\n",
    "            elif j == 1:\n",
    "                Fi = Fi[..., [1, 0]]\n",
    "                Fi[..., 0] = -Fi[..., 0]\n",
    "            elif j == 2:\n",
    "                Fi *= -1\n",
    "            elif j == 3:\n",
    "                Fi = Fi[..., [1, 0]]\n",
    "                Fi[..., 1] = -Fi[..., 1]\n",
    "            F += Fi\n",
    "\n",
    "        if mode == 'density':\n",
    "            if not np.array_equal(res, res[0]*np.ones(len(res))):\n",
    "                print(\"WARNING: density preserving mode not correctly implemented if not all res are equal\")\n",
    "            F *= res[0] ** j\n",
    "        return F\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dF):\n",
    "        V, E, D = ctx.saved_tensors\n",
    "        dV = ddsl_cuda.backward(dF, V, E, D, res, t, j, mode=ctx.mode)\n",
    "        return dV, None, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "0.212738037109375\n"
     ]
    }
   ],
   "source": [
    "dim = 3\n",
    "j = 3\n",
    "npoints = 5\n",
    "r = 64\n",
    "\n",
    "# V, E = create_shape(dim=dim,j=j)\n",
    "V, E = rand_hull(npoints, dim)\n",
    "# V += 1e-3 * torch.rand(V.shape)\n",
    "D = torch.ones(E.shape[0], 1, dtype=V.dtype)\n",
    "\n",
    "res = [r] * dim\n",
    "t = [1] * dim\n",
    "\n",
    "# V, E, D = V.cuda(), E.cuda(), D.cuda()\n",
    "\n",
    "nuft = DDSL.apply\n",
    "from time import time\n",
    "t0 = time()\n",
    "F = nuft(V, E, D, res, t, j, 100, \"density\")\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, E = rand_hull(100, 3)\n",
    "simplex_content(V, E).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = F.squeeze()\n",
    "F_ = F.cpu().detach().numpy()\n",
    "f_ = torch.irfft(F, dim, signal_sizes=res).squeeze().cpu().detach().numpy()\n",
    "print(F_.shape)\n",
    "print(f_.shape)\n",
    "plt.figure()\n",
    "plt.imshow(f_[80, :, :]*8)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
